{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d1d6c5f-5eae-4134-b3d5-a5878e97625b",
   "metadata": {},
   "source": [
    "# **Exercise**\n",
    "> Tackle the **Titanic dataset**. A great place to start is on Kaggle.\n",
    "Alternatively, you can download the data from\n",
    "https://homl.info/titanic.tgz and unzip this tarball like you did for the\n",
    "housing data in Chapter 2. This will give you two CSV files, `train.csv`\n",
    "and `test.csv`, which you can load using `pandas.read_csv()`. The goal is to\n",
    "**train a classifier that can predict the Survived column** based on the other\n",
    "columns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80ebd5-5501-4cab-ac5a-c680200a4b57",
   "metadata": {},
   "source": [
    "### **Step 1**: Load the Dataset\n",
    "We begin by loading `train.csv` and `test.csv`. `train.csv` has both features and the target (Survived), while `test.csv` has only features. We'll train on the former and predict on the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f9cf4d-b3c1-4f50-97f5-aa51386ced0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"datasets/titanic/train.csv\")\n",
    "test_df = pd.read_csv(\"datasets/titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d52096-b244-4426-a5e4-3bd01f95a47a",
   "metadata": {},
   "source": [
    "### **Step 2**: Initial Exploration (EDA)\n",
    "We inspect the structure, check for missing values, and look at distributions. Observations:\n",
    "\n",
    "- `Age`, `Cabin`, and `Embarked` *have missing values*.\n",
    "- `Sex`, `Pclass`, `Embarked` are *categorical*.\n",
    "- We may drop `Cabin`, `Ticket`, and `Name` initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab9a402-fc20-45ba-ad27-33050f2f2fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0    0.616162\n",
       "1    0.383838\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df.describe()\n",
    "train_df.head()\n",
    "train_df[\"Survived\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c7641-b99b-471e-9d68-6b71edda809a",
   "metadata": {},
   "source": [
    "### **Step 3**: Separate Features and Labels\n",
    "We separate the input features (`X_train`) from the target label (`y_train`). We'll train our model using `X_train` to predict `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b92b7b86-6696-49e7-b2ce-7dcd84a0b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop([\"Survived\"], axis=1)\n",
    "y_train = train_df[\"Survived\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fc38f-e4e4-43df-9b20-7ae0544441ea",
   "metadata": {},
   "source": [
    "### **Step 4**: Preprocessing Pipelines\n",
    "We build separate pipelines for:\n",
    "\n",
    "- **Numerical features**: fill missing values using the median, then scale.\n",
    "- **Categorical features**: fill missing values with the most frequent value, then one-hot encode.\n",
    "\n",
    "The `ColumnTransformer` applies each pipeline to the correct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5837ba9-b139-4425-96d1-adbf129e46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Select feature columns\n",
    "num_features = [\"Age\", \"Fare\"]\n",
    "cat_features = [\"Pclass\", \"Sex\", \"Embarked\"]\n",
    "\n",
    "# Numeric pipeline: impute + scale\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: impute + one-hot encode\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine both\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_features),\n",
    "    (\"cat\", cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c224518-4e30-406e-8a86-5336bac545ff",
   "metadata": {},
   "source": [
    "### **Step 5**: Full Modeling Pipeline\n",
    "We wrap **preprocessing** and **classification** into one pipeline. This ensures consistent transformation during training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fa48be-0988-4475-b943-6ada510f2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339bc46e-441a-4858-86b5-d649470f1804",
   "metadata": {},
   "source": [
    "### **Step 6**: Train and Evaluate\n",
    "We use **cross-validation** to estimate accuracy. This helps prevent overfitting and gives a better sense of the modelâ€™s performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6683dd5-3217-4a78-8217-54124077bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.8047517418868871\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(full_pipeline, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "print(\"CV accuracy:\", scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb28caa-57ea-4ca4-8542-6270df9952f4",
   "metadata": {},
   "source": [
    "### **Step 7**: Grid Search for Hyperparameter Tuning\n",
    "We search for the best combination of RandomForest hyperparameters using `GridSearchCV`. The best model will replace the default one in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb52bf9-4aa2-4444-a411-7673019868ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.8271797125102003\n",
      "Best params: {'classifier__max_depth': 10, 'classifier__max_features': 'sqrt', 'classifier__n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"classifier__n_estimators\": [50, 100, 200],\n",
    "    \"classifier__max_depth\": [None, 5, 10],\n",
    "    \"classifier__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "print(\"Best params:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e2eebc-b1e7-4292-8684-6ba64e01c1c4",
   "metadata": {},
   "source": [
    "### **Step 8**: Predict on Test Set\n",
    "We apply the **trained model** on the **real test** set to generate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a2c7965-bf81-421a-b776-91f60d24affe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test features\n",
    "X_test = test_df.copy()\n",
    "passenger_ids = X_test[\"PassengerId\"]\n",
    "X_test = X_test.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# Predict\n",
    "final_model = grid_search.best_estimator_\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# result file\n",
    "submission_df = pd.DataFrame({\n",
    "    \"PassengerId\": passenger_ids,\n",
    "    \"Survived\": y_pred\n",
    "})\n",
    "submission_df.to_csv(\"titanic_submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
